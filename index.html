<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Video Motion Transfer with Diffusion Transformers.">
  <meta name="keywords" content="Video Motion Transfer, Diffusion Transformers, DiTFlow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiTFlow</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÉ</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Video Motion Transfer with Diffusion Transformers</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://alexpondaven.github.io/">Alexander Pondaven</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">Aliaksandr Siarohin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://stulyakov.com/">Sergey Tulyakov</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fabvio.github.io/">Fabio Pizzati</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Oxford,</span>
            <span class="author-block"><sup>2</sup>Snap Inc.,</span>
            <span class="author-block"><sup>3</sup>MBZUAI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.07776"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.07776"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ditflow/ditflow"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="./supp/index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/figures/projectpage_teaser.gif" alt="Teaser GIF" width="100%">
      <h2 class="subtitle has-text-centered">
        <span class="ditflow">DiTFlow</span> transfers motion from a reference video to a new generation without model training
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion Transformers have demonstrated superior performance in video synthesis, however previous motion transfer methods are engineered for UNets or do not take into account the full spatio-temporal information available with DiT's 3D attention mechanism.
          </p>
          <p>
            We propose <span class="ditflow">DiTFlow</span>, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for DiTs.
          </p>
          <p>
            We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). We guide the latent denoising process in an optimization-based, training-free, manner by optimizing latents with our AMF loss to generate videos reproducing the motion of the reference one. We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities. We evaluate <span class="ditflow">DiTFlow</span> against recently published methods, outperforming all across multiple metrics and human evaluation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src=""
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<!-- AMF. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üîç Extracting Motion Signal from DiT</h2>

        <div class="hero-body">
          <img src="./static/figures/amf_extraction.png" alt="AMF Extraction figure" width="100%">
          <!-- <h2 class="subtitle has-text-centered">
            We observe that the cross-frame attention maps of DiTs contain rich motion signals that can be used to guide the latent optimization process.
          </h2> -->
        </div>
        <p>We observe that the cross-frame attention maps of DiTs contain rich motion signals through correspondences between frames. The queries and keys at a given DiT block between a pair of frames can be used to get cross-frame attention. We obtain a displacement vector for each query location by finding the key location with the highest attention score (see paper for details).
        </p>
        <br>
        <p>
          The extracted displacement vectors between each frame pair is visualised below with a color map (e.g. red is rightwards motion of the camera and yellow is downwards motion of the player from frame 0 to frame 4). Feel free to select the first frame using the dropdown menu and use the slider to select the second frame to see the motion flow between the two.</p>

        <div style="min-height: 400px; margin: 20px 0;">
          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-3 has-text-centered">
              <div class="select is-info" style="margin-bottom: 10px;">
                <select id="first-frame-selector">
                  <option value="0">Frame 0</option>
                  <option value="1">Frame 1</option>
                  <option value="2">Frame 2</option>
                  <option value="3">Frame 3</option>
                  <option value="4">Frame 4</option>
                  <option value="5">Frame 5</option>
                </select>
              </div>
              <div id="first-frame-wrapper">
                <img src="./static/figures/amf/ref0.png"
                     class="interpolation-image"
                     alt="First frame"
                     style="width: 100%; height: 300px; object-fit: cover;"/>
              </div>
            </div>
            <div class="column">
              <div style="display: flex; flex-direction: column;">
                <div style="display: flex; justify-content: space-between;">
                  <div style="flex: 1; display: flex; flex-direction: column; align-items: center;">
                    <h4 class="subtitle is-5" style="margin-bottom: 10px;">Target Frame</h4>
                    <div id="reference-image-wrapper" style="height: 300px; width: 300px;">
                      Loading reference...
                    </div>
                  </div>
                  <div style="flex: 1; display: flex; flex-direction: column; align-items: center;">
                    <h4 class="subtitle is-5" style="margin-bottom: 10px;">Motion Flow</h4>
                    <div id="interpolation-image-wrapper" style="height: 300px; width: 300px;">
                      Loading interpolation...
                    </div>
                  </div>
                </div>
                <input class="slider is-fullwidth is-large is-info"
                       id="interpolation-slider"
                       step="1" min="0" max="5" value="0" type="range">
              </div>
            </div>
          </div>
        <!--/ Interpolating. -->
      </div>
    
  </div>
</section>
<!--/ AMF. -->

<!-- Motion Optimization -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">‚öôÔ∏è Motion Optimization</h2>
        <p>We transfer motion during the latent diffusion process by optimizing the noisy latents with a loss between the AMF of the reference and generated frames. These optimization steps occur in the early denoising steps, while later steps are unguided.</p>

        <div class="hero-body">
          <img src="./static/figures/motion_guidance.png" alt="motion guidance diagram" width="100%">
        </div>
        
      </div>
  </div>
</section>
<!-- /Motion Optimization -->

<!-- Zero-shot Motion Injection -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üîÑ Zero-shot Motion Injection</h2>
        <p> An alternative approach is to optimize transformer <i>positional embeddings</i> to transfer motion - intuitively guiding the reorganization of latent patches. <i>Afterwards</i>, we can inject our optimized positional embeddings to a new prompt to generate guided videos without any additional training.
        </p>

        <div class="hero-body">
          <img src="./static/figures/zeroshot_figure.gif" alt="zero shot injection gif" width="100%">
        </div>
        
        <p>Check out <a href="./supp/index.html">Supplementary</a> for more results.</p>
      </div>
  </div>
</section>
<!-- /Zero-shot Motion Injection -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Related Links</h2>

      <div class="content has-text-justified">
        <p>
          Please check out the excellent existing work that tackles the motion transfer problem.
        </p>
        <p>
          <a href="https://diffusion-motion-transfer.github.io/">SMM</a> and <a href="https://arxiv.org/abs/2405.14864v2">MOFT</a> do optimization-based, training-free motion transfer for UNet-based diffusion models using spatially-averaged and motion channel filtered features respectively.
        </p>
        <p>
          Many recent works also try to disentangle motion by training a model conditioned on motion trajectories.
        </p>
      </div>
    </div>
  </div>
  <!--/ Concurrent Work. -->
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{pondaven2024ditflow,
      title={Video Motion Transfer with Diffusion Transformers}, 
      author={Alexander Pondaven and Aliaksandr Siarohin and Sergey Tulyakov and Philip Torr and Fabio Pizzati},
      journal={arXiv preprint arXiv:2412.07776},
      year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/figures/ditflow_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/alexpondaven" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
